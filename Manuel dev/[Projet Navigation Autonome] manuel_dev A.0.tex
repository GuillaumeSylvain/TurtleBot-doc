\documentclass[10pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[nottoc, notlof, notlot]{tocbibind}
\hypersetup{colorlinks=true,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}
\setlength{\parindent}{0.6cm} 
\setlength{\parskip}{0.10cm}
\usepackage[automark]{scrpage2}
\usepackage{color, colortbl}
\usepackage[table]{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}


\pagestyle{scrheadings}

\ihead[]{Équipe Navigation}
\ohead[]{Plan de Développement Qualité}



\begin{document}
\pagestyle {plain}

\begin{titlepage}


\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 

\center

\textsc{\Large Université Paul Sabatier}\\[1cm] 
\includegraphics[scale=0.3]{figures/UPS.jpg}\\[0.6cm] 


\textsc{Master Intelligence Artificielle et \\ 
Reconnaissance des Formes \\ Master Robotique : Décision et Commande}\\[3cm] 

\HRule \\[0.4cm]
{ \huge \bfseries Manuel développeur}\\[0.4cm] 
\LARGE Navigation Autonome de Robot Mobile

\HRule \\[1.5cm]
 

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Auteurs:}\\
\href{mailto:thibaut.aghnatios@laposte.net}{Thibaut \textsc{Aghnatios} }  \\
\href{mailto:bouchetmarinee@gmail.com}{Marine \textsc{Bouchet} } \\
\href{mailto:bruno.dato.meneses@gmail.com}{Bruno \textsc{Dato} } \\
\href{mailto:klempka.tristan@gmail.com}{Tristan \textsc{Klempka} } \\
\href{mailto:lagoute.31@gmail.com}{Thibault \textsc{Lagoute} }  
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Tuteurs:} \\
\href{mailto:lerasle@laas.fr}{Frédéric \textsc{Lerasle}}\\
\href{mailto:michael.lauer@laas.fr}{Michaël \textsc{Lauer}} \\
\href{mailto:taix@laas.f}{Michel \textsc{Taix}}
\end{flushright}
\end{minipage}\\[5cm]

\large 13 mars 2017
 

\end{titlepage}

\newpage


\subsection*{Suivi du document}

\begin{center}
    \begin{tabular}{| l | l | l | l | l |}
    \hline
     \rowcolor{gray} Nom du document & Version Majeure & Version Mineure & Date de création & Dernière version \\ \hline
    Manuel développeur & A & 6 & 13/03/2017 & 1/04/2017 \\ \hline
    \end{tabular}
\end{center}


\subsection*{Auteurs du document}

\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
    \rowcolor{gray} Rédaction & Intégration & Relecture & Validation Interne \\ \hline
    Equipe & ?? & ?? & ?? \\ \hline

    \end{tabular}
\end{center}

\subsection*{Validation du document}

\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
     \rowcolor{gray} Validation & Nom & Date & Visa \\ \hline
    & & & \\
     \hline
    \end{tabular}
\end{center}

\subsection*{Liste de diffusion}

Le rapport du projet est diffusé à l'ensemble des clients et des intervenants externes aux projets.

\subsection*{Historiques de révision}

\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
     \rowcolor{gray} Version & Modification apportée & Auteur & Date \\ \hline
    A.0 & Création du document & Bruno Dato & 13/03/2017\\ \hline
    A.1 & Sections \ref{sec:solution_mise_en_place}, \ref{sec:detection_et_localisation} et \ref{sec:introduction} & Marine Bouchet & 27/03/2017\\ \hline
    A.2 & Section \ref{sec:detection_et_localisation} & Tristan Klempka & 28/03/2017\\ \hline
    A.3 & Section \ref{sec:Visibilite} & Thibaut Aghnatios & 30/03/2017\\ \hline
    A.4 & Sections \ref{sec:commande} et \ref{sec:Superviseur} & Bruno Dato & 30/03/2017\\ \hline
    A.5 & Section \ref{sec:Superviseur} & Bruno Dato & 31/03/2017\\ \hline
    A.6 & Section \ref{sec:introduction} & Bruno Dato & 1/04/2017\\ \hline
     
    \end{tabular}
\end{center}

\newpage
\tableofcontents
\newpage

\section{Introduction}
\label{sec:introduction}

Ce manuel donne les informations nécessaires à l'utilisation et à l'évolution du projet "Navigation autonome d'un robot mobile" du Master 2 IARF et RODECO. Celui-ci permet, à ce jour, d'effectuer une navigation autonome  d'un Turtlebot dans un environnement connu, contenant des amers dont leur position et leur orientation sont connus. Ces indices visuels, visualisés par la kinect, capteur RGB-D, permettent de supprimer les erreurs systématiques de la localisation des capteurs proprioceptifs embarqués -- de l'odométrie. 

La navigation s'effectue entre la position initiale, dont la position est connue ou dans la zone de visibilité d'un amer, et une position finale : 

\begin{center}
\begin{tabular}{c|c|c}
Scénario & A & B \\ \hline
Départ & Position initiale absolue & Position initiale inconnue mais dans une \\ 
  &  & zone de visibilité d'un marqueur \\  \hline
Arrivée & Position finale & Position finale 
\end{tabular}\\
\end{center}

DESCR + deco\\
\begin{tabular}{cl}
B & Si pas de position en entrée  \\
  & Le robot effectue une recherche de marqueur dans son champs visuel \\
  & Tant que pas de marqueur en vue \\
  & Le robot tourne sur lui même \\
  & Il effectue une autre recherche de marqueur dans son champs visuel \\
& \\
A & Tant que le but est plus loin que le prochain marqueur dans sa direction \\
  & Le robot se déplace jusqu'à la zone de visibilité du marqueur le plus proche dans la direction du but  \\
  & Le robot effectue une recherche de marqueur dans son champs visuel \\
  & Tant que pas de marqueur en vue  \\
  & Le robot tourne de sur lui même  \\
  & Il effectue un autre recherche de marqueur dans son champs visuel \\
  & Le robot se déplace jusqu'au but  \\
\end{tabular}

Étant donné que la connaissance sur la visibilité d'un marqueur n'es pas encore opérationnelle, on considèrera que le robot se trouve toujours dans une zone visibilité. On placera alors le robot dans une zone de visibilité à chaque début de navigation en veillant aussi à donner une position initiale correcte.

\section{Navigation avec amers 2D dans un environnement connu}
\label{sec:navigation_avec_amers_2D_dans_un_environnement_connu}

La tâche de navigation autonome se découpe en cinq briques : ++ de textes résumé texte 
\begin{description}
\item [Détection] qui permet de repérer les amers dans le champs de vision quand ils se présentent (cf. AR
\item [Localisation] qui permet de savoir où se trouve le robot, en utilisant soit les données d'odométrie interne, soit les données d’observation (cf. NOUS
\item [Trajectoire] qui permet de se déplacer en boucle fermée d'un point A à un point B (cf. NAV
\item [Commande] "
\item [Visibilité] qui permet de générer une carte contenant les positions des amers et leurs orientations ; et de renvoyer à partir de cette carte le nombre d'amers visibles.
\item [Supervision] qui gère le bon déroulement de la tâche, le déplacement du point de départ au point d'arrivée (cf. 
\end{description}



\subsection{Solution mise en place}
\label{sec:solution_mise_en_place}

L'architecture globale de notre produit est décrite par le schéma suivant :

\begin{figure}[h]
\center
% \includegraphics[scale=0.6]{figures/archi.png} 
\caption{Architecture ROS}	
\end{figure}


\subsection{Détection et localisation}
\label{sec:detection_et_localisation}




\subsubsection{Détection}
\label{sec:detection}

\subsubsection*{Utilisation}

Nous détaillons ici les étapes nécessaires afin d'utiliser le noeud de détection. Ce noeud est lancé indépendamment des autres noeuds du système. 
Le fichier ROS de launch permet de lancer le noeud.\\
\textit{roslaunch localisation.launch}\\

Ce fichier configure et lance la détection d'amer de type AR code à l'aide d'une bibliothèque spécialisée couplée à un nœud ROS. 
Plusieurs paramètres dans ce fichier peuvent être configurés.
\begin{itemize}
\item $marker\_size$ : Largeur (cm) des marqueurs AR utilisés
\item $cam\_image\_topic$ : Topic ROS du flux d'images de la caméra.
\item $cam\_info\_topic$ : Topic ROS des infos propres à la caméra. C'est ici que les données de calibrage sont récupérées.
\item $output\_frame$ : Repère dans lequel sera exprimé le résultat de la détection.\\
\end{itemize}
Des paramètres supplémentaires se trouvent dans le fichier $localication\_node.cpp$. Le changement de ces paramètre nécessite une compilation du noeud ROS.
\begin{itemize}
\item $DEBUG$ : Active ou non les sorties de debug. Si cette option est activée, les transformations intermediaires sont publiées.
\item $TIMEOUT\_AR\_DETEC$ : Temps d'attente maximum pour la reception d'un marqueur. Ce paramètre est utile lors de l'utilisation en réseau. La latence provoque des retards dans la publication des transformées. Il est donc nécessaire de donner une marge de temps au système. Lors de l'utilisation en mode local cette valeur peut \^etre faible.
\item $NB\_MARKER$ : Nombre de marqueurs total dans la scène.\\
\end{itemize}

Une fois le lancement effectué, l'utilisateur peut observé les comportement de ce noeud avec des sorties console.
\begin{itemize}
\item $GLOBAL\_SEARCH$ : La recherche de marqueur a été demandée.
\item $ MARKER DETECTED: ID\_MARKER\_DETECTED$ : Identifiant du marqueur détecté.
\item $LOOKING FOR TF: tf$ :Nom de la transformée attendue.\\
\end{itemize}

\textit{Remarque} : Les performances de la détection des marqueurs reposent sur une bonne calibration de la caméra.







\newpage
\subsubsection*{Fonctionnement}
Afin de détecter les amers dans la scène nous avons recours à la brique ROS : $ar\_track\_alvar$. Cette brique est une interface qui permet d'intégrer à un projet ROS la librairie Alvar. Alvar est une librairie open source pour la détection de marqueur AR.
Cette brique utilise le nuage de points 3D associé à la couleur pour identifier un marqueur. L'information de profondeur permet à l'algorithme de mieux repérer les plans des marqueurs dans la scène. Elle retourne l'identifiant, la position et l'orientation du marqueur dans la position de la $/camera\_rgb\_optical\_frame$, dans notre cas. La librairie fournie 55 AR tags et la possibilité d'entendre cette liste facilement. Dans notre cas, nous utilisons uniquement le noeud qui permet de lire plusieurs marqueurs, un par un, dans un même flux vidéo. 

\lstset{language=XML}
\begin{description}
\item [Mise en place physique] : \\
Lors de notre projet nous avons utilisé des marqueurs de taille $16 \times 16$ cm. On place leur milieu à 31 cm du sol afin que l'axe caméra-marqueur soit le plus parallèle au sol possible.
\item [Initialisation] : \\ Le nœud $ar\_track\_alvar$ s'initialise dans $localisation.launch$ avec les paramètres suivant : 
\begin{lstlisting}
<arg name="marker_size" default="16" />
<arg name="max_new_marker_error" default="0.08" />
<arg name="max_track_error" default="0.2" />

<arg name="cam_image_topic" 
	default="/camera/depth_registered/points" />
<arg name="cam_info_topic" 		
	default="/camera/depth_registered/camera_info" />
<arg name="output_frame" default="/camera_rgb_optical_frame" />
\end{lstlisting}

\item [Lecture du contenu [?]] : \\
	On peut écouter les ar\_track\_alvar\_msgs::AlvarMarkers que publie le nœud avec la commande : 
	\begin{lstlisting} 
	rostopic echo /ar_marker_pose
	\end{lstlisting}
\item [Performances et évolutions] : \\
	L'orientation du markeur trouvé par $ar\_track\_alvar$ doit être : $\vec{z}$ la normale et $\vec{x}$ vers le haut. Il arrive, pour des problèmes inconnus (le réseau ?) que ce repère ne soit pas correctement capturé, avec par exemple, $\vec{y}$ vers le haut. Le robot, à la fin de la localisation se trouve alors couché. Il faudrait alors comprendre d'où vient le problème ou, sinon, ne pas prendre en compte les orientations aberrantes. 
% La détection des marqueurs de $16 \times 16$ cm s'effectue jusqu'à 2m 25 avec une précision de 2 cm et jusqu'à 45\degres d'angle à 5\degres près. Il serait intéressant, à l'avenir, d'approfondir les résultats obtenus avec d'autres tailles si l'erreur est trop aléatoire, ou alors, intégrer un filtre de Kalman pour fusionner les données odométriques et les observations.

DEGRES

\end{description}


\begin{figure}
\center
\includegraphics[scale=0.6]{figures/artags.png} 
\caption{Exemple de marqueurs AR. Les valeurs 3, 4 et 5 sont codés avec ces marqueurs.}	
\end{figure}


\subsubsection{Localisation}
\label{sec:localision}

\subsubsection*{Utilisation}
La node de localisation est lancé gr\^ace à un ficher ROS de launch. Cependant, contrairement au noeud de détection, ce noeud ne se lance pas indépendamment des autres. Il est lancé avec l'ensemble du système de navigation avec la commande suivante.\\
\textit{roslaunch navigation.launch}\\

Ce fichier configure et lance le système de navigation qui comprend la node de localisation. 
On trouve dans ce fichier des paramètres propre à la localisation pouvant être configurés.

\begin{itemize}
\item $map\_file$ : Chemin de la carte utilisée pour la navigation. Notre système utilise le noeud ROS $map\_server$ pour le chargement de la carte.
\item $marker\_tf\_publisher\_X$ : L'utilisateur renseigne ici, pour chaque marqueur, la transformée qui sera publiée pour le repérer sur la carte. L'orientation est renseignée avec un quaternion. \\
\end{itemize}




\noindent Avant toute chose, il est important de rappeler que quelque soit la situation :
\begin{itemize}
\item $/map \rightarrow /odom \rightarrow /base\_link $ (\url{cf. http://www.ros.org/reps/rep-0105.html})
\item ATTENTION AU REPERE MAP [?]
\item /odom est le repère relatif de l'odométrie, assimilé certain, du robot et est placé au départ sur /map. Il est en quelque sorte le point de départ d'un déplacement. Etant donné que ce repère drift au cours du déplacement du robot, il peut être étonné. La localisation via un amer connu permet de placer le repère /odom au bon endroit, et de réinitialiser l'odométrie, ce qui place /base\_link au même endroit. C'est un localisation absolu.
\item chaque frame peut avoir plusieurs fils mais qu'un seul parent
\item la transformé entre deux frames est décrite par deux attributs : 
  \begin{itemize}
  \item $m\_origin$ : Vector3 de translation 
  \item $m\_basis$ : Matrix3x3 de rotation
  \end{itemize}
\end{itemize}

\begin{itemize}

\item [localisation\_node.cpp] :

Ce nœud permet d'envoyer la nouvelle localisation du robot si il détecte un marqueur. 

\noindent La recherche se déclanche uniquement quand la commande haut niveau publie un $<$std\_msgs::Empty$>$ sur le topic $/nav/HLC/askForMarker$. \\

Dans le cas ou un marqueur est visible : \\
\noindent \underline{init} 
/map $\rightarrow$ /odom $\rightarrow$ /baselink $\rightarrow$ /camera\_rgb\_frame $\rightarrow$ /camera\_rgb\_optical\_frame \\
\noindent \underline{arrivée d'un marker} 
/camera\_rgb\_optical\_frame $\rightarrow$ /ar\_marker\_0 \\
\noindent \underline{traitement} \\
\indent /baselink $\rightarrow$ /camera\_rgb\_frame $\rightarrow$ /camera\_rgb\_optical\_frame $\rightarrow$ /ar\_marker\_0 \\
\indent on sait donc /ar\_marker\_0 $\rightarrow$ /baselink 
= /marker\_0 $\rightarrow$ /baselink \\
\indent sachant /map $\rightarrow$ /marker\_0, on sait /map $\rightarrow$ /baselink = /map  $\rightarrow$ /odom \\

\noindent Cette transformation est envoyé sur $/new\_odom$ que l'on publie avec le publisher $odom\_pub$. On publie également l'id du marqueur vu sur $/nav/loca/markerSeen$ sous un $<$std\_msgs::Int16$>$. 

Dans le cas où on ne voit rien on publie : -1 \\

\item [localisation\_broadcaster\_node.cpp] :

Ce nœud permet de tout le temps broadcaster la transfom entre /map et /odom. Il souscrit à la $<$geometry\_msgs::Transform$>$ /new\_odom, qui contient la "nouvelle" position certaine du robot. Dès que celle-ci est publiée, /odom actualise sa position et on réinitialise le nav\_msgs/Odometry : il devient notre nouveau référentiel.
\end{itemize}

\begin{figure}
\center
\includegraphics[scale=0.6]{figures/rqt_loca.png} 
\caption{texte de la légende}	
\end{figure}


\subsection{Commande}
\label{sec:commande}
\subsubsection*{Utilisation}

Pour la version actuelle du projet, la plupart de la commande est effectué par une brique ROS. La brique $move\_base$ réalise la commande afin de suivre la trajectoire générée par la brique trajectoire. Cette brique est lancé dans un fichier ROS de launch.\\
\textit{roslaunch turtlebot$\_$proj$\_$nav navigation.launch}\\
La commande peut être configurée à l'aide du fichier XML : \textit{move\_base.launch.xml}. Ce fichier XML référence un ensemble de fichiers XML qui configure la commande. Une description exhaustive des paramètres de cette brique peut être trouvée à cette adresse :\\
\textit{http://wiki.ros.org/move\_base}\\
Lors de la recherche d'un marqueur, nous reprenons la main et c'est la node $commande\_node$ qui prend le relais. Celle-ci est également lancée dans $navigation.launch$.

\subsubsection*{Fonctionnement}

Au début du projet une ébauche de commande a été réalisée. Celle-ci est simple, elle permet un déplacement en ligne droite et une rotation sur place. Cette commande est en boucle ouverte. Il peut \^etre intéréssant de continuer ce travail afin de prendre en compte une correction odométrique notamment lors d'un évitement d'obstacle lorsque le système reprend la main sur le $navigation\_stack$.\\
Cette ébauche ce trouve dans le fichier $commande\_node.cpp$ \\
Cette node est lancée dans le fichier ROS de launch $navigation.launch$

%TODO explication machine a état

\begin{figure}[!h]
\centering\includegraphics[scale=0.4]{figures/commande_MEF.png}
\caption{Schéma de la machine à états de la commande utilisée pour la recherche d'une balle et pour la recherche d'un amer}
\label{commande_MEF}
\end{figure}

\subsection{Visibilité}
\label{sec:Visibilite}

Le but de cette section est de créer une carte de visibilité, c'est à dire une carte contenant nos différents amers et de retourner le nombre d'amers visibles en fonction de la position du robot.\\
L’intérêt de cette visibilité est d'avoir une carte de visibilité sous RVIZ pour suivre notre navigation d'amers en amers et de savoir si l'on se trouve dans un champ de visibilité pour lancer la détection et la localisation avec notre superviseur.\\
C'est pourquoi, nous allons expliquer comment nous générons notre carte de visibilité à l'aide de $visib\_pgmwriter\_node.cpp$ et notre node retournant le nombre d'amers visibles : $visib\_pgmreader\_node.cpp$.

\subsubsection{Génération de la carte de visibilité}
Tout d'abord pour générer notre carte de visibilité, il faut préalablement avoir une carte de l'environnement (créé virtuellement ou en utilisant la cartographie disponible sur le Turtlebot).\\

\begin{figure}[!h]
\center
\includegraphics[scale=0.6]{figures/aip_map.png} 
\caption{Cartographie de l'environnement avec le Turtlebot}	
\end{figure}

Le node $visib\_pgmwriter\_node.cpp$ ne doit pas être modifié, toutes les configurations se font directement dans le fichier $visib\_init.cpp$. En effet, le node lance la fonction $Ecriture\_carte\_visib()$ qui créé un fichier au format PGM l'ensemble des amers définit dans le $graph.xml$ se trouvant dans le dossier $/rsc$. Donc pour une taille de carte donnée et pour les configurations effectuées correctement dans $visib\_init.cpp$, il suffit de modifier la position et l'orientation de nos amers dans le $graph.xml$ pour que la nouvelle carte de visibilité soit généré automatiquement en lançant à nouveau notre node $visib\_pgmwriter\_node.cpp$.\\

\begin{figure}[!h]
\center
\includegraphics{figures/visib.png} 
\caption{Exemple d'une carte de visibilité}	
\end{figure}

Une fois notre carte générée, il faut pouvoir la visualiser sur Rviz et la superposée avec la carte de notre environnement pour qu'elle soit plus parlante. En lançant le $navigation.launch$ notre carte de visibilité est publiée sur le topic $\/markers\_visibility\_map$. Il suffit de l'ajouter directement sur Rviz en ajoutant une carte et en se plaçant sur ce topic si la configuration de rviz n'est pas enregistrée. Le résultat obtenu sur rviz est le suivant :

\begin{figure}[!h]
\center
\includegraphics[scale=0.4]{figures/visib_rviz.png} 
\caption{Superposition de nos cartes sur rviz}	
\end{figure}

Au niveau de la configuration de $visib\_init.cpp$ dans la fonction $Ecriture\_carte\_visib()$, il faut modifier $int largeur=512;	 int hauteur=640;$ pour que la carte est toujours les mêmes dimensions que la carte de notre environnement. $int nbr_amers=2;$ pour prendre en compte le nombre d'amers dans $graph.xml$ (mettre 2 si 2 amers dans le graphe). Ensuite dans la boucle for faisant la conversion de la position en m en pixels :

$\\
for(a=0;a<nbr\_amers;a++)\\
	\{	\\
		x[a]=(x1[a]+12.2)/0.05;\\
		y[a]=-(y1[a]+13.8-32)/0.05;\\
		alphamax[a]=pi/4;\\
	\}\\
	$

Avant tout, il faut savoir qu'on a 3 repères différents.\\ 
- On a le repère du robot que l'on nomme R et possède la position en x et y du robot donc R (X,Y).\\
- Le repère de la carte de l'environnement note R' qui a pour origine (-12.2 ; -13.8) (dans notre cas pour une carte 512x640) donc R' (X+12.2,Y+13.8).\\
- Ensuite on a le repère de notre carte de visibilité noté Rp qui est positionné en haut a gauche de la carte (vecteur u dans le même sens que x et vecteur v opposé au vecteur y) c'est pourquoi on peut écrire Rp(X+12.2,-(Y+13.8-32)).\\

Explicitons maintenant les différentes valeurs utilisées : 0.05 correspond à la résolution qui se trouve dans le fichier .yaml dans $/map$. Ici 12.2 et 13.8 correspond à l'origine de la carte pour que la superposition des cartes soit correctes. En effet, si on laisse 0;0 la carte se générera a la base du robot sur Rviz donc on aura un décalage lors de la superposition. C'est pourquoi dans le fichier $visib.yaml$, il faut aussi le configurer de cette maniere :\\
$resolution: 0.050000\\
origin: [-12.200000, -13.800000, 0.000000]\\
$
Enfin, la valeur 32 correspond à la conversion pixel/mètres de la hauteur de notre fenêtre soit :\\ $640*0.05=32$.

\subsubsection{Node retournant le nombre d'amers visibles}
Maintenant que nous possédons une carte de visibilité, on va l'utiliser pour renvoyer le nombre d'amers visibles à l'aide de la node $visib\_pgmreader\_node.cpp$. Cette node va aller lire le fichier PGM créé et renvoyer la valeur du pixel pour une position x, y sur cette carte. Si le pixel vaut 15 on est dans une zone blanche donc pas de marqueurs visibles. Pour un pixel de valeur 12, cela veut dire qu'on est dans le champ de visibilité d'un amer. Pour 9, 2 amers visibles et ainsi de suite. Dans notre cas on suppose qu'au maximum 4 amers sont visibles en même temps. Si l'on souhaite plus, il suffit d'augmenter la valeur maximal lors de l’écriture  de notre carte de visibilité. En effet, lors de la création de l’entête PGM, on définit la valeur maximale utilisée (dans notre cas 15).
Au niveau de cette node, elle n'est plus fonctionnelle lors de notre intégration avec le système complet. Lors des tests avant intégration, la node suivante renvoyée bien le nombre d'amers visibles, cependant en l'intégrant notre fonction $pgm\_imread(char *argv)$ ne stocke plus correctement les valeurs des données. Donc cette node est disponible, cependant il faut la débugger pour qu'elle soit utilisée.

\newpage
\subsection{Superviseur}
\label{sec:Superviseur}

\subsubsection{Comportement de la navigation}

Le but du superviseur est de guider le robot pour qu'il atteigne un but de coordonnées $(x,y)$ dans la carte. Afin de gérer le comportement de la navigation à l'aide des amers pour atteindre ce but précis, nous avons mis en place un nœud ROS de supervision : $hightLevelCommand\_node$. Ce nœud permet d'utiliser des commande de haut niveau telles que chercher un amer ou bien se déplacer vers un but.\\

On suppose alors qu'initialement, le robot est dans une position connue a priori ou dans une des zones de visibilité. Si sa position est assez proche du but final suivant un seuil que l'on définira par la suite, on commande alors au robot de se déplacer directement vers le but. Si la position initiale est trop éloignée, on se déplace d'amer en amer pour atteindre l'amer qui est le plus proche du but final et enfin se déplacer vers le but final. Tous les amers et les distances entre ces derniers sont représentés par un graphe, ainsi on utilise l'algorithme de plus court chemin de Dijkstra pour déterminer vers quel amer se déplacer lorsque l'on en a détecté un.\\

Le comportement détaillé du superviseur est décrit par la machine à états figure \ref{HLC_MEF}.

\begin{figure}[!h]
\centering\includegraphics[scale=0.5]{figures/HLC_MEF.pdf}
\caption{Schéma de la machine à états du superviseur de la navigation entre amers}
\label{HLC_MEF}
\end{figure}

Lors de la navigation, quatre signaux sonores permettent de connaitre l'état de la navigation. Ils ont lieu lorsqu'un marqueur est détecté, lorsque le robot a atteint un but intermédiaire ou final et lorsque le robot décroche de sont suivi de trajectoire. Si le robot atteint sont but final avec une précision inférieure à celle demandée au lancement du superviseur, un son supplémentaire est joué.\\

La recherche d'une zone de visibilité n'a pas encore été implémentée. Dans le cas où l'on connait la position initiale du robot dans la carte, il faudrait alors lui commander de se déplacer vers la zone de visibilité la plus proche et alors entamer la recherche de marqueur comme dans le scénario habituel. Dans le cas où cette position est inconnue, il faudrait tout d'abord mettre en place une détection d'obstacles pour pouvoir évoluer dans un environnement inconnu jusqu'à trouver un marqueur.

\subsubsection{Graphe des amers}
\label{sec:grapheDesAmers}

Pour choisir vers quel amer (AR marqueur) se déplacer lorsque l'on en a détecté un et que l'on se trouve encore trop loin du but final pour de diriger directement vers celui-ci, on utilise un graphe représentant tous les marqueurs (figure \ref{graphe}) défini dans un fichier de format XML (figure \ref{graphe_xml}). Chaque nœud du graphe possèdes différentes propriétés : 

\begin{itemize}
\item[•] Id : numéro correspondant au marqueur AR,
\item[•] Label : nom du nœud, chaque nœud correspond à un marqueur AR,
\item[•] PositionX : coordonnée selon l'axe x de la carte connue de l'environnement,
\item[•] PositionY : coordonnée selon l'axe y de la carte,
\item[•] Orientation : angle entre la normale du marqueur AR et l'axe x de la carte (entre 0 et 2$\pi$).
\end{itemize}

Les numéros permettent de se situer dans le graphe lorsque l'on détecte un marqueur et ainsi trouver le marqueur suivant à atteindre pour continuer la navigation. Les coordonnées $(X,Y)$ des marqueurs permet de connaitre les positions à atteindre dans la carte. L'orientation permet de commander au robot de se déplacer en face et à une certaine distance du marqueur pour ne pas foncer dans un mur.

\begin{figure}[!h]
\centering\includegraphics[scale=0.4]{figures/graphe.png}
\caption{Exemple de graphe d'amers (marqueurs AR)}
\label{graphe}
\end{figure}

\begin{figure}[!h]
\centering\includegraphics[scale=0.7]{figures/graphe_xml.png}
\caption{Exemple de définition d'un graphe d'amers (marqueurs AR) en format XML}
\label{graphe_xml}
\end{figure}

\newpage
\subsubsection{Commandes de haut niveau}
\label{sec:commandesDeHautNiveau}

Pour mener à bien la navigation, le superviseur utilise la classe $HighLevelCommand$ qui fournie les commandes de haut niveau suivantes :

\begin{itemize}
\item[•] \textbf{init(threshold)} : L'initialisation permet de définir suivant le seuil (\textbf{threshold}) choisi en paramètre si le robot est trop éloigné ou non du but final pour se servir des marqueurs AR durant la navigation. Si le robot est trop proche, la fonction \textbf{init} renvoie $-1$, le superviseur commandera alors au robot de se déplacer directement vers le but final à l'aide de la fonction \textbf{sendFinalGoal()}. Dans le cas contraire, la fonction cherche le marqueur le plus proche du but final pour naviguer jusqu'à celui-ci avant d'envoyer la dernière commande vers le but final.
\item[•] \textbf{seekMarker()} : Cette fonction permet de commander le comportement de la recherche d'un amer lorsque l'on se trouve dans une zone de visibilité. Tant que cette méthode est appelée et qu'aucun marqueur n'a été détecté, le robot tourne successivement sur lui même des angles $\pi/4$, $-\pi/2$, $3\pi/4$, $-\pi$, $5\pi/4$, $-3\pi/2$ ou $7\pi/4$. Si aucun marqueur n'a été détecté après toutes les rotations, la recherche recommence.
\item[•] \textbf{sendMarkerGoal(distanceToMarker)} : Cette méthode permet de commander au robot de se déplacer en face du marqueur suivant à atteindre pour suivre le plus court chemin vers le but final. La distance à respecter par rapport au marqueur est définie par le paramètre \textbf{distanceToMarker}. 
\item[•] \textbf{sendFinalGoal()} : Cette fonction commande directement le robot vers le but final choisi à l'instanciation de la classe $HighLevelCommand$.
\item[•] \textbf{askForMarker()} :  Cette méthode permet de lancer une rechercher visuelle de marqueur effectuée par le nœud de localisation.
\item[•] \textbf{finalGoal(threshold)} : Permet de jouer une signal sonore spécial lorsque la précision de la position finale du robot par rapport au but fixé est inférieure à un certain seuil défini à l'aide du paramètre \textbf{threshold}.
\end{itemize}

\newpage
\subsubsection{Structure logicielle}
\label{sec:StructureLogicielle}

Afin de communiquer avec les différents nœuds, le nœud de supervision $hightLevelCommand\_node$ est client d'un service, il publie sur quatre topics et il est abonné à quatre autres topics.


\begin{figure}[!h]
\centering\includegraphics[scale=0.6]{figures/HLC_ROS_struc.pdf}
\caption{Schéma de la structure logicielle du nœud de supervision et de ses interactions }
\label{HLC_ROS_struc}
\end{figure}

\subsubsection{Lancement du superviseur}
\label{sec:LancementDuSuperviseur}

Une fois la configuration minimale et les nœuds de navigation lancés, on lance le superviseur dans un terminal à l'aide de la commande suivante :\\

\textbf{rosrun turtlebot$\_$proj$\_$nav highLevelCommand$\_$node X$\_$GOAL Y$\_$GOAL XY$\_$THRESHOLD XY$\_$PRECISION DISTANCE$\_$TO$\_$MARKERS}\\

On choisit alors au lancement du superviseur les paramètres suivants :

\begin{itemize}
\item[•] \textbf{X$\_$GOAL} et \textbf{Y$\_$GOAL} : les coordonnées (X,Y) du but final à atteindre dans la carte,
\item[•] \textbf{XY$\_$THRESHOLD} : le seuil à partir duquel les amers ne sont pas utilisés pour la navigation,
\item[•] \textbf{XY$\_$PRECISION} : la contrainte de précision demandée pour que le signal sonore final soit joué ou non,
\item[•] \textbf{DISTANCE$\_$TO$\_$MARKERS} : la distance par rapport aux marqueurs que le robot doit respecter lorsqu'il se déplace d'amer en amer.
\end{itemize}


\newpage
\section{Conclusion}
\label{sec:conclusion}

\newpage
\listoffigures
\newpage

\section*{ANNEXE}





\end{document}

